
# Реалтайм‑помощник вопросов — Исчерпывающий Overview и Архитектура (v1)

Документ описывает целевую архитектуру мобильного приложения, которое слушает разговор, собирает контекст и по запросу пользователя генерирует список уточняющих вопросов (≈10) на основе LLM. Основа клиента — **Ionic + Angular + Capacitor**, сервер — **.NET 8 + ASP.NET Core + SignalR**, распознавание речи — **Azure Speech Service**.

---

## 1) Цели, сценарии и ограничения

### 1.1 Цели

- Слушать разговор пользователя в реальном времени (микрофон устройства) и преобразовывать речь в текст.

- Поддерживать два режима генерации вопросов:

	- **On‑Demand**: по кнопке «Получить вопросы» выдавать топ‑10 релевантных, разнообразных вопросов.

	- (Опционально) **Реалтайм‑хинты**: лёгкие подсказки без отвлечения.

- Обеспечить приватность, безопасность и управляемую стоимость.

- Обеспечить масштабирование и удобство дальнейшего развития.

### 1.2 Вне рамок (v1)

- Фулл E2E шифрование для облачной LLM (возможен TEE/Confidential VM как компромисс).

- Полноценная он‑девайс LLM (может быть добавлена позже как гибрид).

### 1.3 Пользовательские сценарии (MVP)

- Интервью/собеседование: ведущий слушает, нажимает «Получить вопросы», выбирает и задаёт.

- Встреча с клиентом: во время паузы — получить 10 вариантов и выбрать 2–3 лучшие.

---

## 2) Общая архитектура

```
┌─────────────┐      PCM16 16k   ┌─────────────────────────┐   partial/final   ┌───────────────┐
│  Мобильный  │  ───────────────▶│  ASP.NET Core + SignalR │  ───────────────▶ │   Клиент UI   │
│ Ionic/Ng/Cap│                  │   (Hub: /hubs/asr)      │                  │ (текст + кнопка)
└─────┬───────┘                  └──────────┬──────────────┘                  └──────┬────────┘
      │                                     │                                        │
      │   TLS + (AEAD payload)              │ push PCM16                             │ «Получить
      │                                     ▼                                        │ вопросы»
      │                                ┌────────┐                                   │
      │                                │ Azure  │  partial/final                     │
      │                                │ Speech │◀───────────────────────────────────┘
      │                                └──┬─────┘
      │                                   │ final → LLM trigger (On‑Demand)
      │                                   ▼
      │                            ┌─────────────┐   ranked top‑10   ┌──────────────┐
      │                            │ LLM Service │ ────────────────▶ │ SignalR → UI │
      │                            └─────────────┘                   └──────────────┘
```

Основные компоненты:

- **Клиент**: сбор аудио (AudioWorklet/ScriptProcessor), ресемплинг 48k→16k, упаковка в PCM16 20–40 мс фреймы, отправка в Hub. По кнопке — инициирует генерацию вопросов.

- **Бэкенд (SignalR Hub)**: принимает аудио‑фреймы, пишет в `PushAudioInputStream`, получает partial/final от Azure Speech, хранит «окно» и «роллинг‑сводку». По команде клиента вызывает LLM‑сервис.

- **Azure Speech**: стриминговое распознавание.

- **LLM‑сервис**: генерация кандидатов, реранжирование (diversity/coverage/MMR), возврат топ‑10.

---

## 3) Потоки данных и события

### 3.1 Поток «Слушать»

1. Пользователь жмёт **«Слушать»** → клиент запрашивает `getUserMedia(audio)`.

2. Клиент ресемплит до **16 kHz mono PCM16**, режет на фреймы **20–40 мс**.

3. Отправляет в **SignalR Hub** методом `SendAudioFrame(seq, ts, payload)`.

4. Hub пишет `payload` в `PushAudioInputStream` → **Azure Speech**.

5. События **Recognizing (partial)** и **Recognized (final)** → Hub → клиент (`Partial`, `Final`).

6. Hub поддерживает **скользящее окно** (последние 2–5 мин) и **роллинг‑сводку**.

### 3.2 Поток «Получить вопросы» (On‑Demand)

1. Пользователь жмёт **«Получить вопросы»**.

2. Клиент вызывает Hub метод `GenerateQuestions()` (или `GetQuestions(opts)` с параметрами).

3. Hub «замораживает» контекст: `{rolling_summary, last_window, asked_recently, topic, facts}` и (опционально) псевдонимизирует PII.

4. Hub вызывает **LLM‑сервис**:

		- Генерация `k≈20` кандидатов (oversampling) с тегами/оценками.

		- Реранжирование **MMR (diversity)** + штрафы за дубликаты/историю.

		- Возврат **топ‑10**.

5. Hub шлёт клиенту событие `Questions({data: top10})`.

---

## 4) Контракты сообщений (SignalR)

**Client → Server**

- `SendAudioFrame(int seq, double ts, byte[] payload)` — аудио‑фрейм (PCM16 16k mono). Рекомендуемый размер: 640–1280 байт.

- `StopStream()` — сигнал остановки микрофона (закрыть поток в Speech SDK).

- `GenerateQuestions(object? options = null)` — команда на генерацию списка вопросов.

**Server → Client**

- `Session({ state: "started" | "stream-closed" })`

- `Partial({ text, offset, duration })`

- `Final({ text, offset, duration })`

- `Questions({ data: QuestionItem[] })`

- `Error({ reason, details })`

**Формат аудио‑пакета (если используется бинарный WS‑канал вне SignalR)**

```
[type:1][seq:uint32 LE][ts:float64 LE][payload: bytes of PCM16 16k mono]
```

---

## 5) Модель контекста и управление историей

- **Скользящее окно**: последние **2–5 минут** точной транскрипции.

- **Роллинг‑сводка**: 50–120 слов, обновляется при `final` (recursive summarization).

- **Известные факты (JSON)**: извлечённые сущности/параметры (цели, сроки, бюджет, риски, зависимости).

- **История вопросов**: `asked_recently[]` (для анти‑дубликатов).

- **Диаризация** (опционально): хранить «кто сказал» для каждого фрагмента.

Пример снапшота для LLM:

```json
{
  "topic": "интеграция CRM",
  "rolling_summary": "Клиент хочет связать лиды с сайтом, обсуждаются сроки и бюджет...",
  "last_window": "последние 60–120 секунд диалога...",
  "known_facts": [{"key":"deadline","value":"конец квартала"}],
  "asked_recently": ["Какие сроки?", "Есть ли ограничения по бюджету?"],
  "preferred_style": "деловой, кратко",
  "max_candidates": 20
}
```

---

## 6) LLM: промпт, схема вывода и реранжирование

### 6.1 System‑промпт (суть)

```
Ты — ассистент, формулирующий уместные, лаконичные уточняющие вопросы на основе диалога.
Правила: 0–20 кандидатов, без повторов, разнообразие аспектов (цели/сроки/риски/зависимости/бюджет), осторожность с чувствительными темами. Верни строго JSON по схеме.
```

### 6.2 JSON‑схема ответа

```json
{
  "type":"object",
  "properties":{
    "candidates":{
      "type":"array",
      "items":{
        "type":"object",
        "properties":{
          "text":{"type":"string"},
          "tags":{"type":"array","items":{"type":"string"}},
          "confidence":{"type":"number"},
          "novelty":{"type":"number"}
        },
        "required":["text"]
      }
    }
  },
  "required":["candidates"]
}
```

### 6.3 Реранжирование (MMR + штрафы)

- Счёт: `score = λ*confidence + (1-λ)*novelty − dupPenalty − askedPenalty`.

- **MMR**: при выборе следующего вопроса минимизировать схожесть с уже выбранными (diversity).

- Фильтры: убрать дубликаты/очевидности/нарушители тона.

- Выход: **топ‑10** с отсортированным списком и тегами.

---

## 7) Безопасность и приватность

### 7.1 Транспорт и шифрование

- **TLS 1.3** (HTTP/2/3). Шифры: AES‑GCM (на устройствах с AES‑NI/ARMv8 Crypto) или ChaCha20‑Poly1305.

- Поверх TLS: **AEAD шифрование полезной нагрузки** (libsodium/Tink) с сессионным ключом X25519 (ECDH) + **PFS** (ротация каждые 10–15 мин). Нонсы — счётчик+префикс, допускается GCM‑SIV.

### 7.2 Хранение

- **Клиент**: Keychain/Secure Enclave (iOS), Keystore/StrongBox (Android), данные в **SQLCipher**.

- **Сервер**: диски/бэкапы — AES‑256‑GCM; ключи — **KMS/Key Vault** (envelope encryption).

### 7.3 Обезличивание и политика данных

- Перед LLM — опциональная **псевдонимизация PII** (email/телефоны/адреса/имена).

- Не хранить сырое аудио без необходимости. Хранить сводки и технические логи без PII.

- **Удаление по запросу** и журнал доступа.

### 7.4 Конфиденциальные вычисления (опционально)

- Запуск LLM/ASR внутри **Confidential VM / Intel SGX / AMD SEV‑SNP**. Ключи выдаются только после **удалённой аттестации**.

---

## 8) Масштабирование и производительность

### 8.1 Профиль латентности (ориентиры)

- VAD/захват аудио: 10–100 мс.

- Отправка фреймов: каждые 20–40 мс, батч 40–60 мс.

- Azure Speech partial: каждые 200–500 мс.

- On‑Demand LLM: 300–1500 мс (зависит от контекста/модели).

### 8.2 Масштабирование

- **Azure SignalR Service** как хаб поверх ASP.NET Core Hub.

- Горизонтальное масштабирование воркеров ASR/LLM.

- Очереди/каналы между Hub и ASR (backpressure).

### 8.3 Оптимизации

- Сжатие контекста: `rolling_summary + last_window (≤1200 chars)`.

- Oversampling кандидатов (k≈20) + локальный реранкинг (дёшево).

- Кэширование схожих контекстов на 15–60 секунд.

---

## 9) Надёжность, ошибки и фоллбеки

- **Фоллбек‑логика**: если LLM недоступна — шаблон 4W1H генерирует 5–7 базовых вопросов.

- **Ретраи**: экспоненциальная пауза, идемпотентность по `seq/ts`.

- **Анти‑повторы**: хранить до 50 последних вопросов; фуззи‑сравнение.

- **Rate limiting**: по пользователю/устройству, частота `GenerateQuestions()`.

- **Backpressure**: очередь фреймов; сброс при переполнении с уведомлением UI.

---

## 10) DevOps, конфигурация и инфраструктура

- **CI/CD**: GitHub Actions / Azure DevOps. Артефакты: мобильные билды + контейнеры сервера.

- **IaC**: Bicep/Terraform — ресурсы Speech, App Service/Container Apps, SignalR, Key Vault, Monitor.

- **Мониторинг**: Application Insights (latency, error rate, token cost, WER ASR, Usefulness@k, Diversity@k).

- **Секреты**: только в **Key Vault**, доступ по **Managed Identity**.

---

## 11) UI/UX принципы

- Режим **«Слушать»**: явный индикатор записи, уровень громкости, таймер.

- Кнопка **«Получить вопросы»**: быстрое формирование списка; сверху 3–4 лучших, затем «Показать ещё».

- Фильтры по тегам: сроки/риски/бюджет/зависимости/цели.

- Действия: «Задать», «Переформулировать», «Скрыть» (фидбек влияет на ранжирование).

- Доступность: большие тап‑таргеты, контраст, поддержка VoiceOver/TalkBack.

---

## 12) Тестирование и метрики качества

- **WER (ASR)** по эталонным наборам.

- **Usefulness@k**: доля помеченных как полезные пользователем вопросов.

- **Diversity@k**: количество уникальных тегов/аспектов среди топ‑10.

- **Latency**: P50/P95 для on‑demand генерации.

- **Redundancy**: доля дубликатов (<5%).

Тест‑план: записи из 10–20 реальных сессий, ручная разметка полезности вопросов.

---

## 13) Безопасность и соответствие требованиям

- Экран согласия перед записью: кто обработчик, цель, где хранятся данные, как удалить.

- Индикация записи (световой/иконка). Кнопка «Стоп/Не записывать».

- Политика хранения: срок жизни сводок/логов.

- DPIA/РОPD/локальные требования страны (при необходимости).

---

## 14) Альтернативы и расширения

- **WebRTC** вместо SignalR для медиа (Opus, AEC, адаптивный битрейт). Сложнее сигнальный слой.

- **Он‑девайс LLM** для черновиков (приватность/офлайн), бэкенд — финальная версия.

- **Конфиденциальные вычисления** для LLM/ASR в облаке.

---

## 15) План релиза (инкременты)

**Итерация 1 (MVP)**

- Клиент: запись/стрим PCM16, кнопки «Слушать»/«Получить вопросы».

- Сервер: Hub + Azure Speech, хранение окна/сводки, простая LLM‑генерация (топ‑10 без сложного реранка).

**Итерация 2**

- Псевдонимизация PII, MMR‑реранк, теги/фильтры, анти‑дубликаты.

**Итерация 3**

- Метрики/кэш/стоимость, фоллбеки, конфигурируемые промпты, multi‑lang.

**Итерация 4**

- Диаризация, он‑девайс черновики, A/B‑тесты ранжирования.

---

## 16) Открытые вопросы

- Какие домены/лексика: нужны ли доменные словари/подсказки для ASR?

- Политика хранения: срок жизни транскриптов/сводок, режим «не сохранять вовсе»?

- Минимально приемлемая латентность на on‑demand (SLO)?

- Требуется ли офлайн‑режим (без сети)?

- Нужна ли поддержка групповых переговоров (несколько спикеров/устройств)?

---

## 17) Глоссарий

- **ASR** — автоматическое распознавание речи.

- **LLM** — большая языковая модель.

- **MMR** — Maximal Marginal Relevance, метод диверсификации списка.

- **PII** — персонально идентифицируемая информация.

- **TEE** — доверенная среда исполнения.

---

## 18) Мини‑спецификация API LLM‑сервиса (бэкенд‑внутренний)

`POST /llm/questions`

- **Request**

```json
{
  "topic": "строка",
  "rolling_summary": "строка",
  "last_window": "строка",
  "known_facts": [{"key":"...","value":"..."}],
  "asked_recently": ["..."],
  "preferred_style": "строка",
  "max_candidates": 20
}
```

- **Response**

```json
{
  "candidates": [
    {"text":"...","tags":["сроки"],"confidence":0.78,"novelty":0.62},
    {"text":"..."}
  ]
}
```

После ответа выполняется локальный реранкинг (MMR/штрафы) → топ‑10.

---

## 19) Набор защит и лимитов (рекомендации)

- Максимальная частота `SendAudioFrame`: ≤50 выз/с на соединение.

- Максимальный размер `payload`: ≤4096 байт.

- Частота `GenerateQuestions`: не чаще 1 раз в 4–6 секунд.

- Тайм‑ауты: ASR 60 с бездействия; LLM 10–20 с; общие — circuit‑breaker.

---

## 20) Краткое резюме

Проект строится как легковесный клиент (Ionic/Angular/Capacitor) + сигнальный сервер на .NET 8 (SignalR), стриминговый ASR (Azure Speech), и on‑demand генерация вопросов через LLM‑сервис с реранжированием и обезличиванием. Архитектура безопасна (TLS + AEAD + KMS), масштабируема (Azure SignalR Service, воркеры), и даёт понятные точки роста (диаризация, он‑девайс подсказки, конфиденциальные вычисления).
